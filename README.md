# âš¡ Neural Network Acceleration on GPUs

This project focuses on **accelerating a neural network for MNIST digit classification using CUDA**. Starting from a baseline sequential implementation, we progressively optimize the architecture using GPU computing techniques â€” from naive parallelization to Tensor Core acceleration with cuBLAS and OpenACC.

---

## ğŸ§  Project Overview

The MNIST dataset (handwritten digits 0â€“9) is used as a benchmark for evaluating various neural network implementations. The goal is to **compare multiple versions** of a feedforward neural network to assess improvements in:

- Execution time (training + evaluation)
- Model accuracy
- GPU utilization and profiling metrics

We implement 6 versions:
- **V1**: Sequential CPU
- **V2**: Naive CUDA
- **V3**: Optimized CUDA (with batching, kernel fusion)
- **V4**: Tensor Core using WMMA
- **V5**: OpenACC-based acceleration
- **V6**: cuBLAS + Tensor Core (TF32) acceleration

---

## ğŸ“ Project Structure

```bash
Neural-Network_Acceleration/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ V1/            # Sequential C implementation
â”‚   â”œâ”€â”€ V2/            # Naive CUDA version
â”‚   â”œâ”€â”€ V3/            # Optimized CUDA with batching and kernel fusion
â”‚   â”œâ”€â”€ V4/            # Tensor Core using WMMA (FP16)
â”‚   â”œâ”€â”€ V5/            # OpenACC accelerated version
â”‚   â”œâ”€â”€ V6/            # cuBLAS + TF32 Tensor Core acceleration
â”‚
â”œâ”€â”€ data/              # Raw MNIST dataset (.ubyte files)
â”œâ”€â”€ report/            # Detailed report (PDF)
â”œâ”€â”€ slides/            # Final presentation slides (PPTX)
â””â”€â”€ README.md          # This file
```

## ğŸ“¦ Dataset

We use the official [MNIST dataset](http://yann.lecun.com/exdb/mnist/) containing:

- **60,000** training images
- **10,000** test images
- **784 features per image** (28Ã—28 grayscale)
- **10 output classes** (digits 0â€“9)

Ensure the extracted `.ubyte` files are placed in the `data/` directory.

## âš™ï¸ Setup & Installation

### ğŸ–¥ Prerequisites

- NVIDIA GPU with CUDA support (Compute Capability â‰¥ 7.0 recommended)
- CUDA Toolkit (v11 or newer)
- `make`, `gcc`, `nvcc`, and OpenACC (e.g., PGI or NVIDIA HPC compiler)
- cuBLAS (included with CUDA)

### ğŸ”§ Compile & Run

From the root directory:

```bash
cd src
```

To build and run a specific version:

```bash
make v1      # Sequential
make v2      # Naive CUDA
make v3      # Optimized CUDA
make v4      # Tensor Core (WMMA)
make v5      # OpenACC
make v6      # cuBLAS + Tensor Core
```

To clean all builds:

```bash
make clean
```

## ğŸš€ Implementations & Highlights

### âœ… V1: Sequential (Baseline)
- Pure C implementation with no parallelism
- Single hidden layer (128 neurons, ReLU)
- Softmax output, SGD
- Execution Time: 23.48s
- Test Accuracy: ~97%

### âœ… V2: Naive CUDA
- GPU offload of forward & backward passes
- Kernels: matrixMul, relu, softmax, updateParameters
- Operates per-sample (no batching)
- Execution Time: 37.63s (slower than CPU ğŸ˜¬)
- Test Accuracy: 96.66%
- Speedup: 0.62Ã—

ğŸ” Bottlenecks:
- Excessive memory transfers
- atomicAdd overhead
- Kernel launch latency

### âœ… V3: Optimized CUDA
- Introduces batching, double buffering, and stream-based updates
- Fused kernels, shared memory, warp-level reductions
- Execution Time: 0.675s
- Test Accuracy: 91.25%
- Speedup: 34.79Ã—

ğŸ“Š Batch Size Tuning:

| Batch Size | Time  | Accuracy |
|------------|-------|----------|
| 4          | 5.56s | 96.43%   |
| 32         | 0.87s | 92.52%   |
| 64         | 0.59s | 91.25%   |
| 1024       | 0.35s | 75.62%   |

### âœ… V4: Tensor Core via WMMA
- Uses FP16 matrix multiplication via wmma::mma_sync
- Manual kernel for __half inputs
- Very fast but accuracy drops due to quantization
- Result: Not suitable without mixed precision/loss scaling

### âœ… V5: OpenACC
- Forward pass parallelized with #pragma acc
- Softmax and memory managed via OpenACC
- Backward pass remains on CPU
- Easy to implement but limited acceleration

### âœ… V6: cuBLAS + Tensor Cores (TF32)
- Uses cublasGemmEx with CUBLAS_TF32_TENSOR_OP_MATH
- Combined bias + ReLU kernels
- Shared memory for loss/accuracy aggregation
- Execution Time: 1.033s
- Test Accuracy: 91.30%
- Speedup: 22.73Ã—

## ğŸ“ˆ Version Comparison

| Version | Time (s) | Speedup | Accuracy |
|---------|----------|---------|----------|
| V1      | 23.48    | 1.00Ã—   | ~97%     |
| V2      | 37.63    | 0.62Ã—   | 96.66%   |
| V3      | 0.675    | 34.79Ã—  | 91.25%   |
| V6      | 1.033    | 22.73Ã—  | 91.30%   |

## ğŸ’¡ Learnings & Takeaways

- Batching is essential for GPU throughput
- cuBLAS outperforms manual kernels for larger matrices
- Tensor Cores require precision trade-offs (FP16/TF32)
- OpenACC simplifies code but offers limited control
- Memory layout, stream management, and kernel fusion = key performance levers

## ğŸ§‘â€ğŸ’» Team

| Name           | GitHub           |
|----------------|------------------|
| Saad Nadeem   | @saadnadeem554  |
| Mustafa Iqbal | @Mustafaiqbal2  |
| Hassaan Anwar | @Hassaan-Anwar  |

## ğŸ“ References

- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [cuBLAS Documentation](https://docs.nvidia.com/cuda/cublas/index.html)
- [OpenACC Standard](https://www.openacc.org/)

## ğŸ“„ Report and Slides

- [ğŸ“˜ Project Report (PDF)](./report/report.pdf)
- [ğŸ“Š Presentation Slides (PPTX)](./slides/presentation.pptx)

## github repo link: https://github.com/Mustafaiqbal2/Neural-Network_Acceleration